---
tags:
  - "#RawInformation"
  - "#Paper"
  - "#Unread"
completed: false
title: Paper - Understanding Diffusion Models: A Unified Perspective
authors: "Calvin Luo"
year: '2022/08'
url: "http://arxiv.org/abs/2208.11970"
citekey: "luoUnderstandingDiffusionModels2022"
aliases:
  - "Understanding Diffusion Models"
  - "luoUnderstandingDiffusionModels2022"
---

# Short Summary

## Key Points

# Abstract
```
Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.
```
# Details

## Introduction
- The goal of a **generative model** is to learn to _model_ its true data distribution p(x)
- For many modalities, we can think of the data we observe as represented or generated by an associated unseen latent variable, which we can denote by random variable _z_.
- What is a latent variable
    - Platoâ€™s Allegory of the Cave men seeing shadows: To such people, everything they observe is actually determined by higher-dimensional abstract concepts that they can never behold.
    - Analogously, the objects that we encounter in the actual world may also be generated as a function of some higher-level representations; for example, such representations may encapsulate abstract properties such as color, size, shape, and more. Then, what we observe can be interpreted as a three-dimensional projection or instantiation of such abstract concepts, just as what the cave people observe is actually a two-dimensional projection of three-dimensional objects.
    - cave people can still reason and draw inference about hidden objects
    - in a similar way we can approximate latent representations that describe the data we observe
    - However, unlike cave people we seek low dimentional latent representations, the compress information and can uncover semantically meaningful structure.
        - finding high dimentional representations is fruitless without strong priors

## Evidence lower bounds
- Likelihood based generative modelling: Maximize p(x) of all observed x.
    - We can get p(x) by marginalizing p(x,z)
![[Pasted image 20231106192506.png]]
We can also get p(x) by chain rule
![[Pasted image 20231106192532.png]]

* Both approaches to get p(x) are bad, one involves a integrating over all x, while the other requires access to ground truth latent encoder
- - Evidence lower bounds: a lower bound of the evidence. when it is maximised, it becomes equivalent to the evidence
- maximizing elbo becomes a proxy objective with which to optimize a latent variable model
![[Pasted image 20231106192602.png]]

- $q_\phi(z|x)$ is a flexible approximate variational distribution with parameter $\phi$ that we seek to optimize.
- Intuitively, it can be thought of as a parameterizable model that is learned to estimate the true distribution over latent variables for given observations x, i.e. we seek to resimate the approximate true posterior p(z|x)
- Intuitive Derivation:
![[Pasted image 20231106192642.png]]

We seek to make our variational posterior $q_\phi(z|x)$ match $p(z|x)$ such that the KL divergence is 0. We cannot minimize the KL term because it requires acess to $p(z|x)$,

* - Notice how upto equation 15, the left hand term is a constant, log(p(x)), that does not depend on $\phi$,
    - therefore the increasing elbo decreases KL divergence by an equal amount.
    - therefore elbo is a proxy for the true latent posterior

## Variational Autoencoder
![[Pasted image 20231106192809.png]]
- To maximize ELBO, we must learn two tasks:
![[Pasted image 20231106192827.png]]
* encoder: Learn an intermediate bottlenecking distribution $q_\phi(z|x)$ ; it transforms inputs into a distribution over possible latents
- decoder: learn a deterministic function $p_\theta(x|z)$ to convert a given latent vector $z$ into an observation $x$
- Equation 19
	- - the first term measures the reconstruction likelihood of the decoder from our variational distribution; this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from.
	- The second term measures how similar the learned variational distribution is to a prior belief held over latent variables. Minimizing this term encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function. Maximizing the ELBO is thus equivalent to maximizing its first term and minimizing its second term.
- Encoder is commonly chosen to model a multivariate gaussian with diagonal covariance, and prior a standard multivariate gaussian
![[Pasted image 20231106192956.png]]
- KL divergence term computed analytically, and the reconstruction term approximated with a monte carlo estimate
- Rewrite:
![[Pasted image 20231106193020.png]]
- Why did we pick the $q_\phi(z|x)$ to be multivariate gaussian
    - $z^{(l)}$ is generated by a stochastic sampling procedure, which is generally non-differentiable,
    - solution: make use of the reparamaterization trick
    - reparamaterization trick:
        - Rewrite a random variable as a deterministic function of a noise variable; now you can optimize non-stochastic terms through gradient descent below we reparamaterize X with epsilon
![[Pasted image 20231106193044.png]]
Z is computed as a deterministic function of x and auxiliary noise variable epsilon
![[Pasted image 20231106193117.png]]
You optimize $\mu_\phi(x)$ and $\sigma_\phi(x)$ by computed the gradients with respect to $\phi$
* VAE utilizes reparameterization trick and Monte Carlo to optimize ELBO jointly over \phi and \theta.
- Generating new data
    - sample from latent space p(z)
    - run it through the decoder
    - dimensino of z is often less then that of input x
    - How to control the data generated
        - When a semantically meaningful latent space is learned, latent vectors can be edited before being passed to the decoder

## Hierarchical VAE
- Under this formulation, latent variables themselves are interpreted as generated from other higher-level, more abstract latents.
- In a general HVAE, each latent is allowed to condition on al previous latents, but we study the markovian HVAE
![[Pasted image 20231106193253.png]]
![[Pasted image 20231106193308.png]]


# Comments and Implications

# Links
[@luoUnderstandingDiffusionModels2022]