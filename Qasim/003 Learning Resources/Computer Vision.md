* ViT
	* Emerging Properties in Self-Supervised Vision Transformers 
	* EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation 
	* Systematic Generalization with Edge Transformers
* Diffusion
	* 2800 cite: High-Resolution Image Synthesis with Latent Diffusion Models 
	* Cornernet, ICCV winner 
	* MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation
* NerF
* Multi-task learning
	* Best thesis winner: [https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf](https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf)
* Self-supervised learning
	* https://lilianweng.github.io/posts/2019-11-10-self-supervised/
* Vision + NLP
	* learning transferable visual models from natural language supervision
* History of CNN Architectures
* What is softmax function, when is its gradient high and low, draw a pickture
* What is layernorm, and why use it
* How can you tell if two neural networks have the same architecture, is it by getting the same output
* When you take a derivative of vector with respect to a vector, you should get a matrix, is that what happens in backpropagatoin
* Why convolution size 1
* How do you calculate the complexity of a layer, maximum path length (see attention is all you need)
* why residual connections