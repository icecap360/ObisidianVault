---
tags:
  - "#Ideas"
  - "#Incomplete"
  - "#Software"
topics: "[[Training]]"
---
- Transformers are typically trained with Adam or Adagrad optimizers with very long training schedules and dropout, and this is true for DETR as well.
    - [[Faster R-CNN]], however, is trained with SGD with minimal data augmentation and we are not aware of successful applications of Adam or dropout. Despite these differences we attempt to make a Faster R-CNN baseline stronger (GIOU, random crop, long training).

# Syntax

# Examples

# Comments and Links
- 
# Reference
[[DETR]]