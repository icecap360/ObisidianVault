---
tags:
  - "#Ideas"
topics: "[[GPUs]]"
---


- 2 aspects to consider
	- Data throughput/training time:
		- generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit.
		- If the desired batch size exceeds the limits of the GPU memory, the memory optimization techniques, such as gradient accumulation, can help.
		- However, if the preferred batch size fits into memory, there’s no reason to apply memory-optimizing techniques because they can slow down the training.
	- Model Performance

| Method/tool                                                                                                      | Improves training speed | Optimization Memory Utilization | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ---------------------------------------------------------------------------------------------------------------- | ----------------------- | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Batch size choice](https://huggingface.co/docs/transformers/perf_train_gpu_one#batch-size-choice)               | Yes                     | Yes                             | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [Gradient accumulation](https://huggingface.co/docs/transformers/perf_train_gpu_one#gradient-accumulation)       | No                      | Yes                             | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [Gradient checkpointing](https://huggingface.co/docs/transformers/perf_train_gpu_one#gradient-checkpointing)     | No                      | Yes                             | Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed during the backward pass, would introduce a considerable computational overhead and slow down the training process. Gradient checkpointing offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients.                                        |
| [Mixed precision training](https://huggingface.co/docs/transformers/perf_train_gpu_one#mixed-precision-training) | Yes                     | No                              | While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision.  It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. import torch -- torch.backends.cuda.matmul.allow_tf32 = True -- torch.backends.cudnn.allow_tf32 = True. CUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series. |
| [Optimizer choice](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer-choice)                 | Yes                     | Yes                             | adamw_apex_fused adafactor 8adam multi_tensor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| [Data preloading](https://huggingface.co/docs/transformers/perf_train_gpu_one#data-preloading)                   | Yes                     | No                              | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [DeepSpeed Zero](https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero)                     | No                      | Yes                             | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| torch.compile                                                                                                    | Yes                     | No                              | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                                                                                                                  |                         |                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |


- If these methods do not result in sufficient gains, you can explore the following options:
	- [Look into building your own custom Docker container with efficient softare prebuilds](https://huggingface.co/docs/transformers/perf_train_gpu_one#efficient-software-prebuilds)
	- [Consider a model that uses Mixture of Experts (MoE)](https://huggingface.co/docs/transformers/perf_train_gpu_one#mixture-of-experts)
	- [Convert your model to BetterTransformer to leverage PyTorch native attention](https://huggingface.co/docs/transformers/perf_train_gpu_one#using-pytorch-native-attention)


# Details
- - Automatic Mixed Precision Training
    - If our GPU supports mixed precision training, enabling it is often one of the main ways to boost computational efficiency.
    - In particular, we use automatic mixed precision training, which switches between 32-bit and 16-bit floating point representations during training without sacrificing accuracy.
- Static Graphs with Torch.Compile
    - Under the hood, this is a 3-step process including graph acquisition, graph lowering, and graph compilation.
    - The initial optimization compilation step takes a few minutes but eventually accelerates the model training. In this case, since we are only training the model for three epochs, the benefits of the compilation are not visible due to the extra overhead. However, if we were training the model for longer or training a larger model, the compilation would be worth it.
- Gradient Accumulation
    - Gradient accumulation is a way to virtually increase the batch size during training, which is very useful when the available GPU memory is insufficient to accommodate the desired batch size.
    - gradients are computed for smaller batches and accumulated (usually summed or averaged) over multiple iterations instead of updating the model weights after every batch. Once the accumulated gradients reach the target “virtual” batch size, the model weights are updated with the accumulated gradients.
    - While gradient accumulation can help us train models with larger batch sizes, it does not reduce the total computation required. In fact, it can sometimes lead to a slightly slower training process, as the weight updates are performed less frequently.
# References
[https://huggingface.co/docs/transformers/perf_train_gpu_one](https://huggingface.co/docs/transformers/perf_train_gpu_one)

Details sections:
* [https://lightning.ai/blog/gradient-accumulation/](https://lightning.ai/blog/gradient-accumulation/)
* [https://sebastianraschka.com/blog/2023/pytorch-faster.html](https://sebastianraschka.com/blog/2023/pytorch-faster.html)