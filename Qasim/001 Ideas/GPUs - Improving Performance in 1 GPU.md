
- 2 aspects to consider
	- Data throughput/training time:
		- generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit.
		- If the desired batch size exceeds the limits of the GPU memory, the memory optimization techniques, such as gradient accumulation, can help.
		- However, if the preferred batch size fits into memory, thereâ€™s no reason to apply memory-optimizing techniques because they can slow down the training.
	- Model Performance

| Method/tool                                                                                                      | Improves training speed | Optimization Memory Utilization | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ---------------------------------------------------------------------------------------------------------------- | ----------------------- | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Batch size choice](https://huggingface.co/docs/transformers/perf_train_gpu_one#batch-size-choice)               | Yes                     | Yes                             | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [Gradient accumulation](https://huggingface.co/docs/transformers/perf_train_gpu_one#gradient-accumulation)       | No                      | Yes                             | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [Gradient checkpointing](https://huggingface.co/docs/transformers/perf_train_gpu_one#gradient-checkpointing)     | No                      | Yes                             | Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed during the backward pass, would introduce a considerable computational overhead and slow down the training process. Gradient checkpointing offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients.                                        |
| [Mixed precision training](https://huggingface.co/docs/transformers/perf_train_gpu_one#mixed-precision-training) | Yes                     | No                              | While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision.  It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. import torch -- torch.backends.cuda.matmul.allow_tf32 = True -- torch.backends.cudnn.allow_tf32 = True. CUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series. |
| [Optimizer choice](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer-choice)                 | Yes                     | Yes                             | adamw_apex_fused adafactor 8adam multi_tensor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| [Data preloading](https://huggingface.co/docs/transformers/perf_train_gpu_one#data-preloading)                   | Yes                     | No                              | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [DeepSpeed Zero](https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero)                     | No                      | Yes                             | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| torch.compile                                                                                                    | Yes                     | No                              | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                                                                                                                  |                         |                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |


- If these methods do not result in sufficient gains, you can explore the following options:
	- [Look into building your own custom Docker container with efficient softare prebuilds](https://huggingface.co/docs/transformers/perf_train_gpu_one#efficient-software-prebuilds)
	- [Consider a model that uses Mixture of Experts (MoE)](https://huggingface.co/docs/transformers/perf_train_gpu_one#mixture-of-experts)
	- [Convert your model to BetterTransformer to leverage PyTorch native attention](https://huggingface.co/docs/transformers/perf_train_gpu_one#using-pytorch-native-attention)


# Details
- - Automatic Mixed Precision Training
    - If our GPU supports mixed precision training, enabling it is often one of the main ways to boost computational efficiency.
    - In particular, we use automatic mixed precision training, which switches between 32-bit and 16-bit floating point representations during training without sacrificing accuracy.
    - 
# References
[https://huggingface.co/docs/transformers/perf_train_gpu_one](https://huggingface.co/docs/transformers/perf_train_gpu_one)